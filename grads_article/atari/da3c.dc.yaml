environment:
  run: python environment/training.py
  # List of Atari environments to test:
  # 05 Alien, Amidar, Assault, Asterix, Asteroids,
  # 10 Atlantis, BankHeist, BattleZone, BeamRider, Berzerk,
  # 15 Bowling, Boxing, Breakout, Centipede, ChopperCommand,
  # 20 CrazyClimber, Defender, DemonAttack, DoubleDunk, Enduro,
  # 25 FishingDerby, Freeway, Frostbite, Gopher, Gravitar,
  # 30 Hero, IceHockey, Jamesbond, Kangaroo, Krull,
  # 35 KungFuMaster, MontezumaRevenge, MsPacman, NameThisGame, Phoenix,
  # 40 Pitfall, Pong, PrivateEye, Qbert, Riverraid,
  # 45 RoadRunner, Robotank, Seaquest, Skiing, Solaris,
  # 50 SpaceInvaders, StarGunner, Tennis, TimePilot, Tutankham,
  # 55 UpNDown, Venture, VideoPinball, WizardOfWor, YarsRevenge,
  # 56 Zaxxon
  # missed: Surround
  name: PongDeterministic-v4
  shape: [42,42]
  no_op_max: 30
  stochastic_reset: True
  max_episode_seconds: 1800
  infinite_run: True

relaax-metrics-server:
  enable_unknown_metrics: false
  metrics:
    episode_reward: true
    server_latency: false
    action: false
    critic: true
    state: false

  bind: localhost:7002
  metrics_dir: logs/metrics
  log_metrics_to_console: false
  log_level: INFO  # DEBUG | INFO

relaax-parameter-server:
  bind: localhost:7000
  checkpoint-dir: logs/checkpoints
  checkpoint_step_interval: 1000000
  checkpoints_to_keep: 3
  log-level: INFO  # DEBUG | INFO

relaax-rlx-server:
  bind: localhost:7001
  log-level: INFO  # DEBUG | INFO

version: 1.1.0
algorithm:
  name: da3c
  seed: 0                         # seeds to test: 0, 1e2, 1e5, 1e9, 12345

  input:
    shape: [42, 42]               # state: [height, width] or [height, width, channels]
    history: 1                    # number of consecutive states to stuck to represent an input
    use_convolutions: true        # set to True to use convolutions to process the input,
                                  # it uses the set of convolutions from universe architecture by default
  output:
    continuous: false             # set to True to handle with continuous action type
    action_size: 6                # action size for the given environment

  hidden_sizes: [256]             # list of num_units for hidden fc layers
  batch_size: 20                  # experience size for one update

  use_lstm: true                  # to use LSTM instead of FF, set to the True
  max_global_step: 8e7            # amount of maximum global steps to pass through the training

  rewards_gamma: 0.99             # rewards discount factor
  entropy_beta: 1e-2              # entropy regularization constant

  initial_learning_rate: 1e-4     # initial learning rate to start the training
  use_linear_schedule: false      # set to True to use linear learning rate annealing wrt max_global_step

  optimizer: Adam                 # type of optimizer to use: Adam | RMSProp
  gradients_norm_clipping: 40.    # value to clip the gradients by its global norm

  combine_gradients: dc           # gradient's combining method: dc | fifo | average
  dc_lambda: 0.05                 # delay compensation regularization constant
