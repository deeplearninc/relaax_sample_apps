environment:
  run: python environment/training.py
  name: BipedalWalker-v2
  shape: [24]
  infinite_run: True

relaax-metrics-server:
  enable_unknown_metrics: false
  metrics:
    episode_reward: true
    server_latency: false
    action: false
    critic: true

  bind: localhost:7002
  metrics_dir: logs/metrics
  log_metrics_to_console: false
  log_level: INFO  # DEBUG | INFO

relaax-parameter-server:
  bind: localhost:7000
  checkpoint-dir: logs/checkpoints
  checkpoint_step_interval: 500000
  checkpoints_to_keep: 4
  log-level: INFO  # DEBUG | INFO

relaax-rlx-server:
  bind: localhost:7001
  log-level: INFO  # DEBUG | INFO

version: 1.1.0
algorithm:
  name: trpo
  subtype: ppo

  input:
    shape: [24]                   # shape of input state
    history: 1                    # number of consecutive states to stack
    use_convolutions: false       # set to true to process input by convolution layers

  output:
    continuous: true              # set to true to switch to continuous action space
    action_size: 4                # action size for the given environment

  hidden_sizes: [64, 64]
  activation: tanh                # activation function for MLP
  use_filter: false               # use average filter of the incoming observations and rewards
  async: false                    # set to true to collect experience without blocking the updater

  PG_OPTIONS:
    timestep_limit: 200           # length in steps for one round in environment
    n_iter: 10000                 # number of updates to pass through the training (training length)
    timesteps_per_batch: 5000     # number of experience to collect before update
    rewards_gamma: 0.995          # rewards discount factor
    gae_lambda: 0.97              # lambda from generalized advantage estimation

  PPO:
    clip_e: 0.2                   # clipping parameter for PPO loss function
    learning_rate: 0.001          # learning rate for Adam optimizer used with PPO loss
    n_epochs: 4                   # number of epochs to run on each batch
