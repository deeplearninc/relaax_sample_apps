environment:
  run: python environment/training.py
  name: PongDeterministic-v4
  shape: [42, 42]
  infinite_run: True

relaax-metrics-server:
  enable_unknown_metrics: false
  metrics:
    episode_reward: true
    server_latency: false
    action: false
    mu: false
    sigma2: false
    critic: true
    state: false

  bind: localhost:7002
  metrics_dir: logs/metrics
  log_metrics_to_console: false
  log_level: INFO   # INFO DEBUG

relaax-parameter-server:
  bind: localhost:7000
  checkpoint-dir: logs/checkpoints
  checkpoint_step_interval: 500000
  checkpoints_to_keep: 4
  log-level: INFO   # INFO DEBUG

relaax-rlx-server:
  bind: localhost:7001
  log-level: INFO   # INFO DEBUG

version: 1.1.0
algorithm:
  name: da3c

  input:
    shape: [42, 42]               # shape of input state
    history: 1                    # number of consecutive states to stack
    use_convolutions: true        # set to true to process input by set of convolution layers

  output:
    continuous: false             # set to true to switch to continuous action space
    action_size: 6                # action size for the given environment

  batch_size: 20                  # experience size to perform one update
  hidden_sizes: [256]

  use_lstm: true                  # to use LSTM instead of FF, set to the True
  max_global_step: 1e8            # amount of maximum global steps to pass through the training

  optimizer: Adam
  initial_learning_rate: 1e-4
  combine_gradients: dc           # fifo | avg | dc

  entropy_beta: 0.01              # entropy regularization constant
  rewards_gamma: 0.99             # rewards discount factor
  gradients_norm_clipping: 40.    # gradients clipping by global norm
