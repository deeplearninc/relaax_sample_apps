environment:
  run: python environment/training.py
  name: Pendulum-v0
  max_episodes: 10000
  infinite_run: false

relaax-metrics-server:
  enable_unknown_metrics: false
  metrics:
    episode_reward: true
    server_latency: true
    action: true

  bind: localhost:7002
  metrics_dir: logs/metrics
  log_metrics_to_console: false
  log_level: INFO

relaax-parameter-server:
  bind: localhost:7000
  metrics-dir: logs/metrics
  checkpoint-dir: logs/checkpoints
  log-level: INFO

relaax-rlx-server:
  bind: localhost:7001
  log-level: INFO

version: 1.1.0
algorithm:
  name: trpo
  subtype: trpo-d2

  input:
    shape: [3]
    history: 1
    use_convolutions: false

  output:
    continuous: true
    action_size: 1                # action size for the given environment

  hidden_sizes: [24, 24]
  activation: tanh                # activation function for MLP
  use_filter: false               # use average filter of the incoming observations and rewards
  async: false                    # set to true to collect experience without blocking the updater
  batch_size: 10                  # local loop size for one episode

  use_lstm: false                 # to use LSTM instead of FF, set to the True
  max_global_step: 1e8            # amount of maximum global steps to pass through the training

  initial_learning_rate: 0.02
  entropy_beta: 0.01              # entropy regularization constant
  rewards_gamma: 0.99             # rewards discount factor
  optimizer: RMSProp

  RMSProp:
    decay: 0.99
    epsilon: 0.1
  gradients_norm_clipping: 50.0

  PG_OPTIONS:
    timestep_limit: 200           # length in steps for one round in environment
    n_iter: 10000                 # number of updates to pass through the training (training length)
    timesteps_per_batch: 5000     # number of experience to collect before update
    rewards_gamma: 0.995          # rewards discount factor
    gae_lambda: 0.97              # lambda from generalized advantage estimation

  TRPO:
    cg_damping: 0.1               # multiple of the identity to Fisher matrix during CG
    max_kl: 0.01                  # KL divergence between old and new policy
